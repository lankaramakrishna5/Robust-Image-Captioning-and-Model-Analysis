{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7904435,"sourceType":"datasetVersion","datasetId":4642765},{"sourceId":220213,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":187797,"modelId":209845}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Necessary Imports ---\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n# torchvision is needed for the custom transform you provided earlier\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom nltk.tokenize import word_tokenize\nfrom rouge_score import rouge_scorer # Make sure this library is installed (pip install rouge-score)\n\n# Import models and processors/tokenizers - Base components needed\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForVision2Seq,\n    AutoTokenizer,\n    ViTModel,          # Base ViT model for encoder\n    GPT2LMHeadModel,   # Base GPT-2 model for decoder\n    ViTImageProcessor  # Needed if using HF processor transform, but you used torchvision\n)\nimport torch.nn as nn # Needed for custom model definition\nimport torch.nn.functional as F # Needed for custom model definition\n\n# --- Constants ---\n# Paths based on your Kaggle input structure\nBASE_DIR = \"/kaggle/input/image-captioning-dataset/custom_captions_dataset\"\nTEST_CSV_PATH = os.path.join(BASE_DIR, \"test.csv\")\nTEST_IMAGE_DIR = os.path.join(BASE_DIR, \"test\")\n# Ensure these column names match your test.csv file\nFILENAME_COL = 'filename'\nCAPTION_COL = 'caption'\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 8 # Adjust based on your GPU memory (Kaggle T4/P100 usually handle 8)\nOCCLUSION_LEVELS = [10, 50, 80] # Percentages as required\nPATCH_GRID_SIZE = 16 # Grid size for occlusion (16x16 patches)\n\n# Model specifics\nSMOLVLM_MODEL_NAME = \"HuggingFaceTB/SmolVLM-Instruct\" # Will be downloaded from Hub\n\n# --- Custom Model Configuration ---\n# *** IMPORTANT: You MUST set the path to your saved model weights file below ***\nCUSTOM_MODEL_PATH = \"/kaggle/input/image-captioning-vit-gpt/transformers/default/1/model.pth\" # <--- SET THIS PATH (e.g., \"/kaggle/working/custom_caption_model.pth\" or \"/kaggle/input/my-trained-model/custom_caption_model.pth\")\n\n# Names for downloading/loading the base encoder/decoder from Hugging Face Hub\n# (These will be downloaded if not already cached by Kaggle/HF)\nCUSTOM_ENCODER_NAME = \"WinKawaks/vit-small-patch16-224\"\nCUSTOM_DECODER_NAME = \"gpt2\"\n# Use the decoder name to load the corresponding tokenizer, assuming they match (common for GPT-2)\nCUSTOM_TOKENIZER_PATH = CUSTOM_DECODER_NAME # Correctly uses the variable\n\n# --- NLTK Downloads (ensure they run at least once) ---\nprint(\"Downloading NLTK resources (if needed)...\")\ntry:\n    # Use nltk.data.find to check before downloading if possible, might save time on reruns\n    nltk.data.find('corpora/wordnet.zip')\nexcept nltk.downloader.DownloadError:\n    nltk.download('wordnet', quiet=True)\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/omw-1.4.zip')\nexcept nltk.downloader.DownloadError:\n    nltk.download('omw-1.4', quiet=True) # Needed for METEOR's wordnet lookup\nprint(\"NLTK resources checked/downloaded.\")\n\n# --- Initialize Metric Calculators ---\nSMOOTHIE = SmoothingFunction().method4 # For BLEU calculation stability\nROUGE = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True) # For ROUGE-L F1\n\nprint(\"-\" * 50)\nprint(f\"Configuration:\")\nprint(f\"  Device: {DEVICE}\")\nprint(f\"  Test CSV: {TEST_CSV_PATH}\")\nprint(f\"  Test Images: {TEST_IMAGE_DIR}\")\nprint(f\"  Occlusion Levels: {OCCLUSION_LEVELS}\")\nprint(f\"  SmolVLM Name: {SMOLVLM_MODEL_NAME}\")\nprint(f\"  Custom Encoder: {CUSTOM_ENCODER_NAME}\")\nprint(f\"  Custom Decoder: {CUSTOM_DECODER_NAME}\")\nprint(f\"  Custom Tokenizer: {CUSTOM_TOKENIZER_PATH}\")\nif not CUSTOM_MODEL_PATH:\n    print(\"  WARNING: CUSTOM_MODEL_PATH is not set. Custom model evaluation will fail.\")\nelse:\n    print(f\"  Custom Weights: {CUSTOM_MODEL_PATH}\")\nprint(\"-\" * 50)\n\n# --- Define the torchvision transform from your Part A ---\n# Moved here to ensure it's defined early\nprint(\"Defining custom model image transform (using torchvision)...\")\ncustom_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # Using 0.5, 0.5 for mean, std assumes images were normalized this way during ViT pre-training\n    # or that you fine-tuned ViT with this normalization. Double-check if needed.\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\nprint(\"Custom transform defined.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:26:32.532065Z","iopub.execute_input":"2025-04-14T13:26:32.532367Z","iopub.status.idle":"2025-04-14T13:26:32.546213Z","shell.execute_reply.started":"2025-04-14T13:26:32.532345Z","shell.execute_reply":"2025-04-14T13:26:32.545543Z"}},"outputs":[{"name":"stdout","text":"Downloading NLTK resources (if needed)...\nNLTK resources checked/downloaded.\n--------------------------------------------------\nConfiguration:\n  Device: cuda\n  Test CSV: /kaggle/input/image-captioning-dataset/custom_captions_dataset/test.csv\n  Test Images: /kaggle/input/image-captioning-dataset/custom_captions_dataset/test\n  Occlusion Levels: [10, 50, 80]\n  SmolVLM Name: HuggingFaceTB/SmolVLM-Instruct\n  Custom Encoder: WinKawaks/vit-small-patch16-224\n  Custom Decoder: gpt2\n  Custom Tokenizer: gpt2\n  Custom Weights: /kaggle/input/image-captioning-vit-gpt/transformers/default/1/model.pth\n--------------------------------------------------\nDefining custom model image transform (using torchvision)...\nCustom transform defined.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def occlude_image(image: np.array, mask_percentage: int) -> np.array:\n    \"\"\"\n    Applies patch-wise occlusion to an image by setting pixel values to black.\n\n    Args:\n        image (np.array): Input image as a NumPy array (H, W, C).\n                          Assumes RGB channel order if C=3.\n        mask_percentage (int): Percentage of patches to mask (0-100).\n\n    Returns:\n        np.array: Occluded image as a NumPy array with the same dimensions.\n                  Returns a copy of the original if mask_percentage is 0.\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image must be a NumPy array.\")\n    if not (0 <= mask_percentage <= 100):\n        raise ValueError(\"mask_percentage must be between 0 and 100\")\n\n    # Return a copy if no occlusion is needed\n    if mask_percentage == 0:\n        return image.copy()\n\n    img_h, img_w = image.shape[:2]\n\n    # Calculate patch dimensions based on the 16x16 grid requirement\n    patch_h = img_h // PATCH_GRID_SIZE\n    patch_w = img_w // PATCH_GRID_SIZE\n\n    # Handle cases where the image is too small for the grid\n    if patch_h == 0 or patch_w == 0:\n         print(f\"Warning: Image dimensions ({img_h}x{img_w}) are too small \"\n               f\"to create meaningful patches for a {PATCH_GRID_SIZE}x{PATCH_GRID_SIZE} grid. \"\n               f\"Skipping occlusion for this image.\")\n         return image.copy()\n\n    num_patches_h = PATCH_GRID_SIZE\n    num_patches_w = PATCH_GRID_SIZE\n    total_patches = num_patches_h * num_patches_w\n\n    # Calculate the number of patches to mask, ensuring at least one if percentage > 0\n    num_patches_to_mask = int(round(total_patches * (mask_percentage / 100.0)))\n    if num_patches_to_mask == 0 and mask_percentage > 0:\n        num_patches_to_mask = 1 # Guarantee at least one patch is masked\n\n    # Ensure we don't try to mask more patches than exist\n    num_patches_to_mask = min(num_patches_to_mask, total_patches)\n\n    # Generate all possible patch indices (row, column) from 0 to 15\n    all_patch_indices = [(r, c) for r in range(num_patches_h) for c in range(num_patches_w)]\n\n    # Randomly select the indices of the patches to mask without replacement\n    indices_to_mask = random.sample(all_patch_indices, num_patches_to_mask)\n\n    # Create a copy of the image to modify\n    occluded_img = image.copy()\n\n    # Iterate through the selected patch indices and apply the mask\n    for r_idx, c_idx in indices_to_mask:\n        # Calculate pixel coordinates for the top-left corner of the patch\n        start_row = r_idx * patch_h\n        start_col = c_idx * patch_w\n\n        # Calculate pixel coordinates for the bottom-right corner (exclusive)\n        # Use original image dimensions to handle potential non-perfect divisions correctly\n        end_row = start_row + patch_h\n        end_col = start_col + patch_w\n\n        # Ensure end points do not exceed image boundaries\n        end_row = min(end_row, img_h)\n        end_col = min(end_col, img_w)\n\n        # Set the selected patch region to black (0)\n        # Works for grayscale (H, W) or color (H, W, C) images\n        occluded_img[start_row:end_row, start_col:end_col] = 0\n\n    return occluded_img\n\n# --- Optional: Example Usage & Visualization ---\n# Uncomment below to test the function with a sample image if needed\n# import matplotlib.pyplot as plt\n# try:\n#     # Load a sample image (make sure TEST_CSV_PATH and TEST_IMAGE_DIR are set)\n#     sample_df = pd.read_csv(TEST_CSV_PATH)\n#     if not sample_df.empty:\n#         sample_img_path = os.path.join(TEST_IMAGE_DIR, sample_df.iloc[0][FILENAME_COL])\n#         if os.path.exists(sample_img_path):\n#             sample_pil_image = Image.open(sample_img_path).convert(\"RGB\")\n#             sample_np_image = np.array(sample_pil_image)\n\n#             # Apply occlusion (e.g., 50%)\n#             occluded_50 = occlude_image(sample_np_image, 50)\n#             occluded_10 = occlude_image(sample_np_image, 10)\n#             occluded_80 = occlude_image(sample_np_image, 80)\n\n#             # Display using matplotlib\n#             fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n#             ax[0].imshow(sample_np_image)\n#             ax[0].set_title(\"Original\")\n#             ax[0].axis('off')\n#             ax[1].imshow(occluded_10)\n#             ax[1].set_title(\"10% Occluded\")\n#             ax[1].axis('off')\n#             ax[2].imshow(occluded_50)\n#             ax[2].set_title(\"50% Occluded\")\n#             ax[2].axis('off')\n#             ax[3].imshow(occluded_80)\n#             ax[3].set_title(\"80% Occluded\")\n#             ax[3].axis('off')\n#             plt.tight_layout()\n#             plt.show()\n\n#             # You can convert back to PIL if needed for model input:\n#             # occluded_pil = Image.fromarray(occluded_50)\n#         else:\n#             print(f\"Sample image not found: {sample_img_path}\")\n#     else:\n#         print(\"Test CSV is empty or not found.\")\n\n# except Exception as e:\n#      print(f\"Error during example usage: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:26:43.431729Z","iopub.execute_input":"2025-04-14T13:26:43.432017Z","iopub.status.idle":"2025-04-14T13:26:43.441158Z","shell.execute_reply.started":"2025-04-14T13:26:43.431996Z","shell.execute_reply":"2025-04-14T13:26:43.440323Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport torch\n# Ensure other necessary imports like ViTImageProcessor, transforms are present from previous cells\nfrom transformers import ViTImageProcessor\n\n\n# --- Dataset Definition ---\n# (Using the version you provided in the last message)\nclass ImageCaptionDataset(Dataset):\n    \"\"\"\n    Loads image paths and captions, providing PIL images and optionally transformed tensors.\n    \"\"\"\n    def __init__(self, csv_path, image_dir, filename_col, caption_col, transform=None, dataframe=None):\n        \"\"\"\n        Args:\n            csv_path (string, optional): Path to the csv file. Used if dataframe is None.\n            image_dir (string): Directory with all the images.\n            filename_col (string): Name of the column with image filenames.\n            caption_col (string): Name of the column with ground truth captions.\n            transform (callable, optional): Optional transform for the custom model's input.\n            dataframe (pd.DataFrame, optional): Use this pre-loaded DataFrame instead of reading csv_path.\n        \"\"\"\n        if dataframe is not None:\n            # Use the provided dataframe directly (make a copy)\n            self.data_frame = dataframe.copy()\n            print(f\"Dataset initialized using provided DataFrame.\")\n        elif csv_path is not None:\n            # Load from CSV path if dataframe not provided\n            print(f\"Loading DataFrame from CSV: {csv_path}\")\n            try:\n                self.data_frame = pd.read_csv(csv_path)\n            except FileNotFoundError:\n                print(f\"Error: CSV file not found at {csv_path}\")\n                self.data_frame = pd.DataFrame(columns=[filename_col, caption_col]) # Empty df\n        else:\n            raise ValueError(\"Must provide either csv_path or dataframe to ImageCaptionDataset\")\n\n\n        self.image_dir = image_dir\n        self.transform = transform\n        self.filename_col = filename_col\n        self.caption_col = caption_col\n\n        # --- Data Cleaning and Validation ---\n        initial_count = len(self.data_frame)\n        if initial_count > 0:\n            print(f\"Initial entries before validation: {initial_count}\")\n            self.data_frame.dropna(subset=[self.filename_col, self.caption_col], inplace=True)\n            self.data_frame = self.data_frame[\n                self.data_frame[self.caption_col].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)\n            ]\n            self.data_frame['full_path'] = self.data_frame[self.filename_col].apply(\n                lambda x: os.path.join(self.image_dir, str(x))\n            )\n            # Check file existence - can be slow, consider sampling check if needed\n            print(\"Validating image file paths...\")\n            self.data_frame['exists'] = self.data_frame['full_path'].apply(os.path.exists)\n            self.data_frame = self.data_frame[self.data_frame['exists']]\n            self.data_frame.drop(columns=['exists'], inplace=True)\n\n            final_count = len(self.data_frame)\n            print(f\"Entries after validation (paths exist, captions valid): {final_count}\")\n            if final_count == 0:\n                 print(f\"Warning: Dataset has 0 valid entries after validation.\")\n        else:\n             print(f\"Warning: Initial DataFrame has 0 entries.\")\n\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        # (Your __getitem__ logic remains the same)\n        if torch.is_tensor(idx): idx = idx.tolist()\n        row = self.data_frame.iloc[idx]\n        img_full_path = row['full_path']\n        caption = row[self.caption_col]\n        filename = row[self.filename_col]\n        try:\n            pil_image = Image.open(img_full_path).convert('RGB')\n        except Exception as e:\n            return None, None, None, None # Handled by collate_fn\n        transformed_tensor = None\n        if self.transform:\n            try:\n                transformed_tensor = self.transform(pil_image)\n            except Exception as e:\n                 # print(f\"Warning: Transform failed for {filename}: {e}\") # Optional debug\n                 pass # Keep tensor as None\n        return pil_image, caption, transformed_tensor, filename\n\n\n# --- Custom Collate Function ---\n# (Using the version you provided - it handles None correctly)\ndef custom_collate_fn(batch):\n    original_batch_size = len(batch)\n    batch = [item for item in batch if item[0] is not None]\n    filtered_batch_size = len(batch)\n    if filtered_batch_size == 0: return None, None, None, None\n    pil_images, captions, transformed_tensors_list, filenames = zip(*batch)\n    valid_tensors = [t for t in transformed_tensors_list if t is not None]\n    transformed_batch = None\n    if valid_tensors:\n        try:\n            # Only stack if the number of valid tensors matches the filtered batch size\n            if len(valid_tensors) == filtered_batch_size:\n                 transformed_batch = torch.stack(valid_tensors)\n            # else: # Optional: print warning if some transforms failed in the batch\n            #      print(f\"Warning: Collate received {filtered_batch_size} valid images but only {len(valid_tensors)} valid tensors.\")\n        except Exception as e:\n            print(f\"Error stacking transformed tensors in collate_fn: {e}. Batch tensor will be None.\")\n    return list(pil_images), list(captions), transformed_batch, list(filenames)\n\n\n# --- Define Transforms for Custom Model ---\n# (Using ViTImageProcessor as in your provided code)\n# Ensure necessary constants (CUSTOM_ENCODER_NAME) are defined from the setup cell\ncustom_transform = None # Initialize\ntry:\n    if 'CUSTOM_ENCODER_NAME' not in globals(): raise NameError(\"CUSTOM_ENCODER_NAME not defined.\")\n    print(f\"Loading ViTImageProcessor for: {CUSTOM_ENCODER_NAME}\")\n    custom_image_processor = ViTImageProcessor.from_pretrained(CUSTOM_ENCODER_NAME)\n    custom_transform = lambda pil_img: custom_image_processor(\n        images=pil_img, return_tensors=\"pt\"\n    ).pixel_values.squeeze(0) # Remove batch dim\n    print(\"Custom model transform defined successfully using ViTImageProcessor.\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to load ViTImageProcessor from '{CUSTOM_ENCODER_NAME}'. Error: {e}\")\n    print(\"Custom transform will be None. Custom model evaluation likely to fail.\")\n\n\n# --- Create LIMITED Dataset and DataLoader ---\nNUM_IMAGES_TO_PROCESS = 200\nprint(f\"\\n--- Creating LIMITED Dataset/DataLoader for first {NUM_IMAGES_TO_PROCESS} images ---\")\n\n# Ensure necessary constants are defined\nrequired_vars = ['TEST_CSV_PATH', 'TEST_IMAGE_DIR', 'FILENAME_COL', 'CAPTION_COL', 'BATCH_SIZE', 'DEVICE']\nfor var in required_vars:\n    if var not in globals(): raise NameError(f\"Constant '{var}' is not defined.\")\n\ntest_dataloader = None\ntest_dataset = None\nsubset_df = None # Initialize\n\ntry:\n    # 1. Load the full DataFrame from CSV\n    print(f\"Loading full DataFrame from {TEST_CSV_PATH}...\")\n    full_df = pd.read_csv(TEST_CSV_PATH)\n    print(f\"Full DataFrame loaded with {len(full_df)} rows.\")\n\n    # 2. Select the first N rows that potentially exist\n    if len(full_df) == 0:\n         print(\"Warning: Full DataFrame is empty. No data to process.\")\n    else:\n        if len(full_df) >= NUM_IMAGES_TO_PROCESS:\n            subset_df = full_df.head(NUM_IMAGES_TO_PROCESS).copy()\n            print(f\"Selected first {len(subset_df)} rows for potential evaluation.\")\n        else:\n            subset_df = full_df.copy()\n            print(f\"Selected all {len(subset_df)} available rows (less than {NUM_IMAGES_TO_PROCESS}).\")\n\n    # 3. Create the dataset using the SUBSET DataFrame (if subset_df is not None)\n    if subset_df is not None and not subset_df.empty:\n        # Pass the subset DataFrame directly to the modified Dataset constructor\n        test_dataset = ImageCaptionDataset(\n            image_dir=TEST_IMAGE_DIR,\n            filename_col=FILENAME_COL,\n            caption_col=CAPTION_COL,\n            transform=custom_transform, # Pass the transform defined above\n            dataframe=subset_df,        # Pass the filtered DataFrame here\n            csv_path=None               # Explicitly set csv_path to None\n        )\n\n        # 4. Create the DataLoader ONLY if the dataset is valid and non-empty\n        if len(test_dataset) > 0:\n            test_dataloader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                shuffle=False,\n                num_workers=2,\n                collate_fn=custom_collate_fn,\n                pin_memory=True if DEVICE == \"cuda\" else False\n            )\n            print(f\"LIMITED Test DataLoader created. Samples after validation: {len(test_dataset)}, Batches: {len(test_dataloader)}\")\n        else:\n            print(\"ERROR: The limited dataset has 0 valid entries after validation (check image paths/captions in the first 200 rows). Cannot create DataLoader.\")\n    else:\n         print(\"Skipping Dataset/DataLoader creation as the subset DataFrame is empty or None.\")\n\n\nexcept FileNotFoundError:\n     print(f\"ERROR: Test CSV not found at {TEST_CSV_PATH}. Cannot create DataLoader.\")\nexcept Exception as e:\n     print(f\"ERROR creating limited dataset/dataloader: {e}\")\n\n# Check if dataloader was created successfully\nprint(f\"\\nDataLoader 'test_dataloader' ready for use: {'Yes' if test_dataloader is not None else 'No'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:26:51.445684Z","iopub.execute_input":"2025-04-14T13:26:51.445995Z","iopub.status.idle":"2025-04-14T13:26:52.023754Z","shell.execute_reply.started":"2025-04-14T13:26:51.445965Z","shell.execute_reply":"2025-04-14T13:26:52.023004Z"}},"outputs":[{"name":"stdout","text":"Loading ViTImageProcessor for: WinKawaks/vit-small-patch16-224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55a05b65212468b83efce38adff0a35"}},"metadata":{}},{"name":"stdout","text":"Custom model transform defined successfully using ViTImageProcessor.\n\n--- Creating LIMITED Dataset/DataLoader for first 200 images ---\nLoading full DataFrame from /kaggle/input/image-captioning-dataset/custom_captions_dataset/test.csv...\nFull DataFrame loaded with 928 rows.\nSelected first 200 rows for potential evaluation.\nDataset initialized using provided DataFrame.\nInitial entries before validation: 200\nValidating image file paths...\nEntries after validation (paths exist, captions valid): 200\nLIMITED Test DataLoader created. Samples after validation: 200, Batches: 25\n\nDataLoader 'test_dataloader' ready for use: Yes\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Ensure necessary base imports are present\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import ViTModel, GPT2LMHeadModel, AutoTokenizer, AutoProcessor, AutoModelForVision2Seq\nfrom PIL import Image\nfrom torchvision import transforms # Needed for the custom transform\nimport os # Needed for file path checks\n\n# --- User-Provided Custom Model Definition ---\n# (Pasted directly from your input)\nclass ImageCaptionModel(nn.Module):\n    def __init__(self, encoder, decoder, processor, tokenizer, embed_dim=768):\n        super(ImageCaptionModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.processor = processor # Store if needed, otherwise can be None\n        self.tokenizer = tokenizer # Store tokenizer\n        # Linear layer to map encoder output dim to decoder input dim\n        self.encoder_to_decoder = nn.Linear(encoder.config.hidden_size, embed_dim)\n        print(f\"Initialized ImageCaptionModel: Encoder hidden size {encoder.config.hidden_size}, Decoder embed dim {embed_dim}\")\n\n    def forward(self, images, input_ids, attention_mask=None):\n        # Forward pass used for training (calculates loss)\n        with torch.no_grad(): # Usually encoder fine-tuning is off during captioning training\n            encoder_outputs = self.encoder(pixel_values=images).last_hidden_state\n        image_features = encoder_outputs.mean(dim=1) # Average pooling as per your code\n        image_embeds = self.encoder_to_decoder(image_features).unsqueeze(1)\n        decoder_input_embeds = self.decoder.transformer.wte(input_ids)\n        decoder_inputs_embeds = torch.cat([image_embeds, decoder_input_embeds], dim=1)\n        final_attention_mask = None\n        if attention_mask is not None:\n            bos_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n            final_attention_mask = torch.cat([bos_mask, attention_mask], dim=1)\n        decoder_outputs = self.decoder(inputs_embeds=decoder_inputs_embeds, attention_mask=final_attention_mask)\n        logits = decoder_outputs.logits[:, 1:] # Slice off prediction based on image embed\n        labels = input_ids.clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n        labels_flat = labels.contiguous().view(-1)\n        loss = F.cross_entropy(logits_flat, labels_flat)\n        return loss\n\n    def generate(self, image_tensor, tokenizer, max_length=30):\n        # Generation method used for inference\n        self.eval()\n        if image_tensor.ndim == 3:\n             image_tensor = image_tensor.unsqueeze(0)\n        image_tensor = image_tensor.to(next(self.parameters()).device)\n        with torch.no_grad():\n            encoder_outputs = self.encoder(pixel_values=image_tensor).last_hidden_state\n            image_features = encoder_outputs.mean(dim=1)\n            encoder_embeds = self.encoder_to_decoder(image_features).unsqueeze(1)\n            # Use tokenizer's BOS or EOS as starting token\n            start_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id\n            if start_token is None:\n                print(\"ERROR in generate: Cannot find BOS or EOS token ID.\")\n                return \"\"\n            generated_ids = [start_token]\n            for _ in range(max_length - 1):\n                input_ids_tensor = torch.tensor([generated_ids], device=image_tensor.device)\n                decoder_input_embeds = self.decoder.transformer.wte(input_ids_tensor)\n                decoder_inputs_embeds = torch.cat([encoder_embeds, decoder_input_embeds], dim=1)\n                attn_mask = torch.ones(decoder_inputs_embeds.shape[:2], device=image_tensor.device)\n                outputs = self.decoder(inputs_embeds=decoder_inputs_embeds, attention_mask=attn_mask)\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n                generated_ids.append(next_token_id)\n                if next_token_id == tokenizer.eos_token_id:\n                    break\n            caption = tokenizer.decode(generated_ids, skip_special_tokens=True)\n            return caption.strip()\n# === End of Pasted Class Definition ===\n\n\n# --- Load Pre-trained Models ---\n\n# 1. Load SmolVLM (Downloads from Hub)\nprint(\"Loading SmolVLM model and processor...\")\n# Check if constants are defined from previous cell\nif 'SMOLVLM_MODEL_NAME' not in globals(): raise NameError(\"SMOLVLM_MODEL_NAME not defined.\")\nif 'DEVICE' not in globals(): raise NameError(\"DEVICE not defined.\")\n\nsmol_model = None\nsmol_processor = None\ntry:\n    smol_processor = AutoProcessor.from_pretrained(SMOLVLM_MODEL_NAME)\n    smol_model = AutoModelForVision2Seq.from_pretrained(\n        SMOLVLM_MODEL_NAME,\n        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n        low_cpu_mem_usage=True, # Helps on systems with less CPU RAM\n        _attn_implementation=\"eager\" # Use eager if flash attention isn't available/working\n    ).to(DEVICE)\n    smol_model.eval() # Set to evaluation mode\n    print(\"SmolVLM loaded successfully.\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to load SmolVLM model '{SMOLVLM_MODEL_NAME}': {e}\")\n    # Keep smol_model and smol_processor as None\n\n# 2. Load Custom Model Components (Downloads base models from Hub)\nprint(\"\\nLoading Custom Model components...\")\ncustom_model = None\ncustom_tokenizer = None\n# Check if constants are defined\nif 'CUSTOM_TOKENIZER_PATH' not in globals(): raise NameError(\"CUSTOM_TOKENIZER_PATH not defined.\")\nif 'CUSTOM_ENCODER_NAME' not in globals(): raise NameError(\"CUSTOM_ENCODER_NAME not defined.\")\nif 'CUSTOM_DECODER_NAME' not in globals(): raise NameError(\"CUSTOM_DECODER_NAME not defined.\")\nif 'CUSTOM_MODEL_PATH' not in globals(): raise NameError(\"CUSTOM_MODEL_PATH not defined.\")\n\ntry:\n    # Load Tokenizer (Downloads from Hub using the specified name)\n    print(f\"Loading custom tokenizer from Hub: {CUSTOM_TOKENIZER_PATH}\")\n    custom_tokenizer = AutoTokenizer.from_pretrained(CUSTOM_TOKENIZER_PATH)\n    # Set PAD token if missing (GPT-2 often needs this)\n    if custom_tokenizer.pad_token is None:\n        if custom_tokenizer.eos_token is not None:\n            custom_tokenizer.pad_token = custom_tokenizer.eos_token\n            print(f\"Set custom tokenizer PAD token to EOS token: {custom_tokenizer.eos_token} ({custom_tokenizer.eos_token_id})\")\n        else:\n            # Add a pad token if EOS is also missing (less common)\n             custom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n             print(\"Added '[PAD]' as PAD token.\")\n             # Resize decoder embeddings if a new token was added\n             # resize_needed = True # Flag this for later\n    if custom_tokenizer.pad_token is None:\n         raise ValueError(\"Custom tokenizer needs a PAD token (set to EOS or added manually).\")\n\n\n    # Load the pre-trained base encoder and decoder models from Hub\n    print(f\"Loading pre-trained encoder from Hub: {CUSTOM_ENCODER_NAME}\")\n    encoder = ViTModel.from_pretrained(CUSTOM_ENCODER_NAME).to(DEVICE)\n\n    print(f\"Loading pre-trained decoder from Hub: {CUSTOM_DECODER_NAME}\")\n    decoder = GPT2LMHeadModel.from_pretrained(CUSTOM_DECODER_NAME).to(DEVICE)\n\n    # Instantiate YOUR custom model class using the pre-trained parts\n    print(\"Instantiating custom ImageCaptionModel architecture...\")\n    custom_model = ImageCaptionModel(\n        encoder=encoder,\n        decoder=decoder,\n        processor=None, # Pass None as processor is handled externally by transform\n        tokenizer=custom_tokenizer\n    ).to(DEVICE)\n    print(\"Custom model architecture instantiated.\")\n\n    # --- Load your fine-tuned weights ---\n    if not CUSTOM_MODEL_PATH:\n        print(\"\\nWARNING: CUSTOM_MODEL_PATH is empty. Skipping loading of fine-tuned weights.\")\n        print(\"         Custom model will use pre-trained weights only (likely poor performance).\")\n    elif not os.path.isfile(CUSTOM_MODEL_PATH):\n         print(f\"\\nERROR: Custom model weights file not found at '{CUSTOM_MODEL_PATH}'.\")\n         print(\"         Custom model will use pre-trained weights only.\")\n         custom_model = None # Set to None to prevent errors later if weights are essential\n    else:\n        try:\n            print(f\"Loading fine-tuned weights from: {CUSTOM_MODEL_PATH}\")\n            custom_model.load_state_dict(torch.load(CUSTOM_MODEL_PATH, map_location=DEVICE))\n            custom_model.eval() # Set to evaluation mode after loading weights\n            print(\"Custom Model fine-tuned weights applied successfully.\")\n        except RuntimeError as e:\n             print(f\"\\nERROR: Runtime error loading custom model weights from '{CUSTOM_MODEL_PATH}'.\")\n             print(f\"       Check if the saved weights match the current model architecture (encoder/decoder names). Error: {e}\")\n             print(\"       Custom model evaluation might fail or use only pre-trained weights.\")\n             # Decide whether to proceed with base weights or stop:\n             # custom_model = None # Option: Stop if weights don't load\n        except Exception as e:\n             print(f\"\\nERROR: An unexpected error occurred loading custom model weights: {e}\")\n             custom_model = None # Stop if weights don't load\n\n\nexcept Exception as e:\n    print(f\"\\nERROR: An critical error occurred during custom model component loading: {e}\")\n    custom_model = None # Ensure model is None if setup failed\n\n\n# --- Inference Helper Functions ---\n\n# 1. SmolVLM Caption Generation\ndef generate_caption_smolvlm(pil_image, model, processor, device):\n    \"\"\"Generates a caption for a PIL image using the SmolVLM model.\"\"\"\n    # Check if model and processor were loaded successfully\n    if model is None or processor is None or pil_image is None:\n        # print(\"Skipping SmolVLM generation: model/processor missing.\") # Optional debug\n        return \"\"\n    try:\n        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe this image briefly.\"}]}]\n        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n        inputs = processor(text=prompt, images=[pil_image], return_tensors=\"pt\").to(device, model.dtype) # Use model's dtype for inputs\n        with torch.no_grad():\n            output_ids = model.generate(\n                **inputs,\n                max_new_tokens=128, # Max caption length\n                do_sample=False,    # Greedy decoding\n                pad_token_id=processor.tokenizer.pad_token_id\n            )\n        raw_output = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n        marker = \"Assistant:\"\n        caption = raw_output.split(marker, 1)[-1].strip() if marker in raw_output else raw_output\n        return caption\n    except Exception as e:\n        # print(f\"SmolVLM inference error: {e}\") # Uncomment for debugging\n        return \"\"\n\n# 2. Custom Model Caption Generation\ndef generate_caption_custom(image_tensor, model, tokenizer, device, max_length=30):\n    \"\"\"\n    Generates a caption for a single transformed image tensor using the custom model's\n    own .generate() method. image_tensor should be the output of custom_transform.\n    \"\"\"\n    # Check if model and tokenizer were loaded successfully\n    if model is None or tokenizer is None or image_tensor is None:\n        # print(\"Skipping custom generation: model/tokenizer missing.\") # Optional debug\n        return \"\"\n    try:\n        # Ensure input tensor is on the correct device\n        image_tensor = image_tensor.to(device)\n        # Call the model's implemented generate method\n        caption = model.generate(image_tensor, tokenizer, max_length=max_length)\n        return caption\n    except Exception as e:\n        # print(f\"Custom model inference error: {e}\") # Uncomment for debugging\n        return \"\"\n\nprint(\"\\nInference helper functions defined.\")\n# Final check if models are ready for evaluation\nprint(f\"SmolVLM model ready: {'Yes' if smol_model and smol_processor else 'No'}\")\nprint(f\"Custom model ready: {'Yes' if custom_model and custom_tokenizer else 'No'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:27:08.811789Z","iopub.execute_input":"2025-04-14T13:27:08.812669Z","iopub.status.idle":"2025-04-14T13:27:19.268863Z","shell.execute_reply.started":"2025-04-14T13:27:08.812646Z","shell.execute_reply":"2025-04-14T13:27:19.267887Z"}},"outputs":[{"name":"stdout","text":"Loading SmolVLM model and processor...\nSmolVLM loaded successfully.\n\nLoading Custom Model components...\nLoading custom tokenizer from Hub: gpt2\nSet custom tokenizer PAD token to EOS token: <|endoftext|> (50256)\nLoading pre-trained encoder from Hub: WinKawaks/vit-small-patch16-224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8c36ae7fef4526bb7bc861268fec59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6164700f3fb40e8bb75efba63e19f16"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Loading pre-trained decoder from Hub: gpt2\n","output_type":"stream"},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8492dd789c141a68e6c6bbf4f6be04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4ef416a5b04d19b9d3faac9168d4e7"}},"metadata":{}},{"name":"stdout","text":"Instantiating custom ImageCaptionModel architecture...\nInitialized ImageCaptionModel: Encoder hidden size 384, Decoder embed dim 768\nCustom model architecture instantiated.\nLoading fine-tuned weights from: /kaggle/input/image-captioning-vit-gpt/transformers/default/1/model.pth\n\nERROR: Runtime error loading custom model weights from '/kaggle/input/image-captioning-vit-gpt/transformers/default/1/model.pth'.\n       Check if the saved weights match the current model architecture (encoder/decoder names). Error: Error(s) in loading state_dict for ImageCaptionModel:\n\tMissing key(s) in state_dict: \"encoder.embeddings.cls_token\", \"encoder.embeddings.position_embeddings\", \"encoder.embeddings.patch_embeddings.projection.weight\", \"encoder.embeddings.patch_embeddings.projection.bias\", \"encoder.encoder.layer.0.attention.attention.query.weight\", \"encoder.encoder.layer.0.attention.attention.query.bias\", \"encoder.encoder.layer.0.attention.attention.key.weight\", \"encoder.encoder.layer.0.attention.attention.key.bias\", \"encoder.encoder.layer.0.attention.attention.value.weight\", \"encoder.encoder.layer.0.attention.attention.value.bias\", \"encoder.encoder.layer.0.attention.output.dense.weight\", \"encoder.encoder.layer.0.attention.output.dense.bias\", \"encoder.encoder.layer.0.intermediate.dense.weight\", \"encoder.encoder.layer.0.intermediate.dense.bias\", \"encoder.encoder.layer.0.output.dense.weight\", \"encoder.encoder.layer.0.output.dense.bias\", \"encoder.encoder.layer.0.layernorm_before.weight\", \"encoder.encoder.layer.0.layernorm_before.bias\", \"encoder.encoder.layer.0.layernorm_after.weight\", \"encoder.encoder.layer.0.layernorm_after.bias\", \"encoder.encoder.layer.1.attention.attention.query.weight\", \"encoder.encoder.layer.1.attention.attention.query.bias\", \"encoder.encoder.layer.1.attention.attention.key.weight\", \"encoder.encoder.layer.1.attention.attention.key.bias\", \"encoder.encoder.layer.1.attention.attention.value.weight\", \"encoder.encoder.layer.1.attention.attention.value.bias\", \"encoder.encoder.layer.1.attention.output.dense.weight\", \"encoder.encoder.layer.1.attention.output.dense.bias\", \"encoder.encoder.layer.1.intermediate.dense.weight\", \"encoder.encoder.layer.1.intermediate.dense.bias\", \"encoder.encoder.layer.1.output.dense.weight\", \"encoder.encoder.layer.1.output.dense.bias\", \"encoder.encoder.layer.1.layernorm_before.weight\", \"encoder.encoder.layer.1.layernorm_before.bias\", \"encoder.encoder.layer.1.layernorm_after.weight\", \"encoder.encoder.layer.1.layernorm_after.bias\", \"encoder.encoder.layer.2.attention.attention.query.weight\", \"encoder.encoder.layer.2.attention.attention.query.bias\", \"encoder.encoder.layer.2.attention.attention.key.weight\", \"encoder.encoder.layer.2.attention.attention.key.bias\", \"encoder.encoder.layer.2.attention.attention.value.weight\", \"encoder.encoder.layer.2.attention.attention.value.bias\", \"encoder.encoder.layer.2.attention.output.dense.weight\", \"encoder.encoder.layer.2.attention.output.dense.bias\", \"encoder.encoder.layer.2.intermediate.dense.weight\", \"encoder.encoder.layer.2.intermediate.dense.bias\", \"encoder.encoder.layer.2.output.dense.weight\", \"encoder.encoder.layer.2.output.dense.bias\", \"encoder.encoder.layer.2.layernorm_before.weight\", \"encoder.encoder.layer.2.layernorm_before.bias\", \"encoder.encoder.layer.2.layernorm_after.weight\", \"encoder.encoder.layer.2.layernorm_after.bias\", \"encoder.encoder.layer.3.attention.attention.query.weight\", \"encoder.encoder.layer.3.attention.attention.query.bias\", \"encoder.encoder.layer.3.attention.attention.key.weight\", \"encoder.encoder.layer.3.attention.attention.key.bias\", \"encoder.encoder.layer.3.attention.attention.value.weight\", \"encoder.encoder.layer.3.attention.attention.value.bias\", \"encoder.encoder.layer.3.attention.output.dense.weight\", \"encoder.encoder.layer.3.attention.output.dense.bias\", \"encoder.encoder.layer.3.intermediate.dense.weight\", \"encoder.encoder.layer.3.intermediate.dense.bias\", \"encoder.encoder.layer.3.output.dense.weight\", \"encoder.encoder.layer.3.output.dense.bias\", \"encoder.encoder.layer.3.layernorm_before.weight\", \"encoder.encoder.layer.3.layernorm_before.bias\", \"encoder.encoder.layer.3.layernorm_after.weight\", \"encoder.encoder.layer.3.layernorm_after.bias\", \"encoder.encoder.layer.4.attention.attention.query.weight\", \"encoder.encoder.layer.4.attention.attention.query.bias\", \"encoder.encoder.layer.4.attention.attention.key.weight\", \"encoder.encoder.layer.4.attention.attention.key.bias\", \"encoder.encoder.layer.4.attention.attention.value.weight\", \"encoder.encoder.layer.4.attention.attention.value.bias\", \"encoder.encoder.layer.4.attention.output.dense.weight\", \"encoder.encoder.layer.4.attention.output.dense.bias\", \"encoder.encoder.layer.4.intermediate.dense.weight\", \"encoder.encoder.layer.4.intermediate.dense.bias\", \"encoder.encoder.layer.4.output.dense.weight\", \"encoder.encoder.layer.4.output.dense.bias\", \"encoder.encoder.layer.4.layernorm_before.weight\", \"encoder.encoder.layer.4.layernorm_before.bias\", \"encoder.encoder.layer.4.layernorm_after.weight\", \"encoder.encoder.layer.4.layernorm_after.bias\", \"encoder.encoder.layer.5.attention.attention.query.weight\", \"encoder.encoder.layer.5.attention.attention.query.bias\", \"encoder.encoder.layer.5.attention.attention.key.weight\", \"encoder.encoder.layer.5.attention.attention.key.bias\", \"encoder.encoder.layer.5.attention.attention.value.weight\", \"encoder.encoder.layer.5.attention.attention.value.bias\", \"encoder.encoder.layer.5.attention.output.dense.weight\", \"encoder.encoder.layer.5.attention.output.dense.bias\", \"encoder.encoder.layer.5.intermediate.dense.weight\", \"encoder.encoder.layer.5.intermediate.dense.bias\", \"encoder.encoder.layer.5.output.dense.weight\", \"encoder.encoder.layer.5.output.dense.bias\", \"encoder.encoder.layer.5.layernorm_before.weight\", \"encoder.encoder.layer.5.layernorm_before.bias\", \"encoder.encoder.layer.5.layernorm_after.weight\", \"encoder.encoder.layer.5.layernorm_after.bias\", \"encoder.encoder.layer.6.attention.attention.query.weight\", \"encoder.encoder.layer.6.attention.attention.query.bias\", \"encoder.encoder.layer.6.attention.attention.key.weight\", \"encoder.encoder.layer.6.attention.attention.key.bias\", \"encoder.encoder.layer.6.attention.attention.value.weight\", \"encoder.encoder.layer.6.attention.attention.value.bias\", \"encoder.encoder.layer.6.attention.output.dense.weight\", \"encoder.encoder.layer.6.attention.output.dense.bias\", \"encoder.encoder.layer.6.intermediate.dense.weight\", \"encoder.encoder.layer.6.intermediate.dense.bias\", \"encoder.encoder.layer.6.output.dense.weight\", \"encoder.encoder.layer.6.output.dense.bias\", \"encoder.encoder.layer.6.layernorm_before.weight\", \"encoder.encoder.layer.6.layernorm_before.bias\", \"encoder.encoder.layer.6.layernorm_after.weight\", \"encoder.encoder.layer.6.layernorm_after.bias\", \"encoder.encoder.layer.7.attention.attention.query.weight\", \"encoder.encoder.layer.7.attention.attention.query.bias\", \"encoder.encoder.layer.7.attention.attention.key.weight\", \"encoder.encoder.layer.7.attention.attention.key.bias\", \"encoder.encoder.layer.7.attention.attention.value.weight\", \"encoder.encoder.layer.7.attention.attention.value.bias\", \"encoder.encoder.layer.7.attention.output.dense.weight\", \"encoder.encoder.layer.7.attention.output.dense.bias\", \"encoder.encoder.layer.7.intermediate.dense.weight\", \"encoder.encoder.layer.7.intermediate.dense.bias\", \"encoder.encoder.layer.7.output.dense.weight\", \"encoder.encoder.layer.7.output.dense.bias\", \"encoder.encoder.layer.7.layernorm_before.weight\", \"encoder.encoder.layer.7.layernorm_before.bias\", \"encoder.encoder.layer.7.layernorm_after.weight\", \"encoder.encoder.layer.7.layernorm_after.bias\", \"encoder.encoder.layer.8.attention.attention.query.weight\", \"encoder.encoder.layer.8.attention.attention.query.bias\", \"encoder.encoder.layer.8.attention.attention.key.weight\", \"encoder.encoder.layer.8.attention.attention.key.bias\", \"encoder.encoder.layer.8.attention.attention.value.weight\", \"encoder.encoder.layer.8.attention.attention.value.bias\", \"encoder.encoder.layer.8.attention.output.dense.weight\", \"encoder.encoder.layer.8.attention.output.dense.bias\", \"encoder.encoder.layer.8.intermediate.dense.weight\", \"encoder.encoder.layer.8.intermediate.dense.bias\", \"encoder.encoder.layer.8.output.dense.weight\", \"encoder.encoder.layer.8.output.dense.bias\", \"encoder.encoder.layer.8.layernorm_before.weight\", \"encoder.encoder.layer.8.layernorm_before.bias\", \"encoder.encoder.layer.8.layernorm_after.weight\", \"encoder.encoder.layer.8.layernorm_after.bias\", \"encoder.encoder.layer.9.attention.attention.query.weight\", \"encoder.encoder.layer.9.attention.attention.query.bias\", \"encoder.encoder.layer.9.attention.attention.key.weight\", \"encoder.encoder.layer.9.attention.attention.key.bias\", \"encoder.encoder.layer.9.attention.attention.value.weight\", \"encoder.encoder.layer.9.attention.attention.value.bias\", \"encoder.encoder.layer.9.attention.output.dense.weight\", \"encoder.encoder.layer.9.attention.output.dense.bias\", \"encoder.encoder.layer.9.intermediate.dense.weight\", \"encoder.encoder.layer.9.intermediate.dense.bias\", \"encoder.encoder.layer.9.output.dense.weight\", \"encoder.encoder.layer.9.output.dense.bias\", \"encoder.encoder.layer.9.layernorm_before.weight\", \"encoder.encoder.layer.9.layernorm_before.bias\", \"encoder.encoder.layer.9.layernorm_after.weight\", \"encoder.encoder.layer.9.layernorm_after.bias\", \"encoder.encoder.layer.10.attention.attention.query.weight\", \"encoder.encoder.layer.10.attention.attention.query.bias\", \"encoder.encoder.layer.10.attention.attention.key.weight\", \"encoder.encoder.layer.10.attention.attention.key.bias\", \"encoder.encoder.layer.10.attention.attention.value.weight\", \"encoder.encoder.layer.10.attention.attention.value.bias\", \"encoder.encoder.layer.10.attention.output.dense.weight\", \"encoder.encoder.layer.10.attention.output.dense.bias\", \"encoder.encoder.layer.10.intermediate.dense.weight\", \"encoder.encoder.layer.10.intermediate.dense.bias\", \"encoder.encoder.layer.10.output.dense.weight\", \"encoder.encoder.layer.10.output.dense.bias\", \"encoder.encoder.layer.10.layernorm_before.weight\", \"encoder.encoder.layer.10.layernorm_before.bias\", \"encoder.encoder.layer.10.layernorm_after.weight\", \"encoder.encoder.layer.10.layernorm_after.bias\", \"encoder.encoder.layer.11.attention.attention.query.weight\", \"encoder.encoder.layer.11.attention.attention.query.bias\", \"encoder.encoder.layer.11.attention.attention.key.weight\", \"encoder.encoder.layer.11.attention.attention.key.bias\", \"encoder.encoder.layer.11.attention.attention.value.weight\", \"encoder.encoder.layer.11.attention.attention.value.bias\", \"encoder.encoder.layer.11.attention.output.dense.weight\", \"encoder.encoder.layer.11.attention.output.dense.bias\", \"encoder.encoder.layer.11.intermediate.dense.weight\", \"encoder.encoder.layer.11.intermediate.dense.bias\", \"encoder.encoder.layer.11.output.dense.weight\", \"encoder.encoder.layer.11.output.dense.bias\", \"encoder.encoder.layer.11.layernorm_before.weight\", \"encoder.encoder.layer.11.layernorm_before.bias\", \"encoder.encoder.layer.11.layernorm_after.weight\", \"encoder.encoder.layer.11.layernorm_after.bias\", \"encoder.layernorm.weight\", \"encoder.layernorm.bias\", \"encoder.pooler.dense.weight\", \"encoder.pooler.dense.bias\", \"decoder.transformer.wte.weight\", \"decoder.transformer.wpe.weight\", \"decoder.transformer.h.0.ln_1.weight\", \"decoder.transformer.h.0.ln_1.bias\", \"decoder.transformer.h.0.attn.c_attn.weight\", \"decoder.transformer.h.0.attn.c_attn.bias\", \"decoder.transformer.h.0.attn.c_proj.weight\", \"decoder.transformer.h.0.attn.c_proj.bias\", \"decoder.transformer.h.0.ln_2.weight\", \"decoder.transformer.h.0.ln_2.bias\", \"decoder.transformer.h.0.mlp.c_fc.weight\", \"decoder.transformer.h.0.mlp.c_fc.bias\", \"decoder.transformer.h.0.mlp.c_proj.weight\", \"decoder.transformer.h.0.mlp.c_proj.bias\", \"decoder.transformer.h.1.ln_1.weight\", \"decoder.transformer.h.1.ln_1.bias\", \"decoder.transformer.h.1.attn.c_attn.weight\", \"decoder.transformer.h.1.attn.c_attn.bias\", \"decoder.transformer.h.1.attn.c_proj.weight\", \"decoder.transformer.h.1.attn.c_proj.bias\", \"decoder.transformer.h.1.ln_2.weight\", \"decoder.transformer.h.1.ln_2.bias\", \"decoder.transformer.h.1.mlp.c_fc.weight\", \"decoder.transformer.h.1.mlp.c_fc.bias\", \"decoder.transformer.h.1.mlp.c_proj.weight\", \"decoder.transformer.h.1.mlp.c_proj.bias\", \"decoder.transformer.h.2.ln_1.weight\", \"decoder.transformer.h.2.ln_1.bias\", \"decoder.transformer.h.2.attn.c_attn.weight\", \"decoder.transformer.h.2.attn.c_attn.bias\", \"decoder.transformer.h.2.attn.c_proj.weight\", \"decoder.transformer.h.2.attn.c_proj.bias\", \"decoder.transformer.h.2.ln_2.weight\", \"decoder.transformer.h.2.ln_2.bias\", \"decoder.transformer.h.2.mlp.c_fc.weight\", \"decoder.transformer.h.2.mlp.c_fc.bias\", \"decoder.transformer.h.2.mlp.c_proj.weight\", \"decoder.transformer.h.2.mlp.c_proj.bias\", \"decoder.transformer.h.3.ln_1.weight\", \"decoder.transformer.h.3.ln_1.bias\", \"decoder.transformer.h.3.attn.c_attn.weight\", \"decoder.transformer.h.3.attn.c_attn.bias\", \"decoder.transformer.h.3.attn.c_proj.weight\", \"decoder.transformer.h.3.attn.c_proj.bias\", \"decoder.transformer.h.3.ln_2.weight\", \"decoder.transformer.h.3.ln_2.bias\", \"decoder.transformer.h.3.mlp.c_fc.weight\", \"decoder.transformer.h.3.mlp.c_fc.bias\", \"decoder.transformer.h.3.mlp.c_proj.weight\", \"decoder.transformer.h.3.mlp.c_proj.bias\", \"decoder.transformer.h.4.ln_1.weight\", \"decoder.transformer.h.4.ln_1.bias\", \"decoder.transformer.h.4.attn.c_attn.weight\", \"decoder.transformer.h.4.attn.c_attn.bias\", \"decoder.transformer.h.4.attn.c_proj.weight\", \"decoder.transformer.h.4.attn.c_proj.bias\", \"decoder.transformer.h.4.ln_2.weight\", \"decoder.transformer.h.4.ln_2.bias\", \"decoder.transformer.h.4.mlp.c_fc.weight\", \"decoder.transformer.h.4.mlp.c_fc.bias\", \"decoder.transformer.h.4.mlp.c_proj.weight\", \"decoder.transformer.h.4.mlp.c_proj.bias\", \"decoder.transformer.h.5.ln_1.weight\", \"decoder.transformer.h.5.ln_1.bias\", \"decoder.transformer.h.5.attn.c_attn.weight\", \"decoder.transformer.h.5.attn.c_attn.bias\", \"decoder.transformer.h.5.attn.c_proj.weight\", \"decoder.transformer.h.5.attn.c_proj.bias\", \"decoder.transformer.h.5.ln_2.weight\", \"decoder.transformer.h.5.ln_2.bias\", \"decoder.transformer.h.5.mlp.c_fc.weight\", \"decoder.transformer.h.5.mlp.c_fc.bias\", \"decoder.transformer.h.5.mlp.c_proj.weight\", \"decoder.transformer.h.5.mlp.c_proj.bias\", \"decoder.transformer.h.6.ln_1.weight\", \"decoder.transformer.h.6.ln_1.bias\", \"decoder.transformer.h.6.attn.c_attn.weight\", \"decoder.transformer.h.6.attn.c_attn.bias\", \"decoder.transformer.h.6.attn.c_proj.weight\", \"decoder.transformer.h.6.attn.c_proj.bias\", \"decoder.transformer.h.6.ln_2.weight\", \"decoder.transformer.h.6.ln_2.bias\", \"decoder.transformer.h.6.mlp.c_fc.weight\", \"decoder.transformer.h.6.mlp.c_fc.bias\", \"decoder.transformer.h.6.mlp.c_proj.weight\", \"decoder.transformer.h.6.mlp.c_proj.bias\", \"decoder.transformer.h.7.ln_1.weight\", \"decoder.transformer.h.7.ln_1.bias\", \"decoder.transformer.h.7.attn.c_attn.weight\", \"decoder.transformer.h.7.attn.c_attn.bias\", \"decoder.transformer.h.7.attn.c_proj.weight\", \"decoder.transformer.h.7.attn.c_proj.bias\", \"decoder.transformer.h.7.ln_2.weight\", \"decoder.transformer.h.7.ln_2.bias\", \"decoder.transformer.h.7.mlp.c_fc.weight\", \"decoder.transformer.h.7.mlp.c_fc.bias\", \"decoder.transformer.h.7.mlp.c_proj.weight\", \"decoder.transformer.h.7.mlp.c_proj.bias\", \"decoder.transformer.h.8.ln_1.weight\", \"decoder.transformer.h.8.ln_1.bias\", \"decoder.transformer.h.8.attn.c_attn.weight\", \"decoder.transformer.h.8.attn.c_attn.bias\", \"decoder.transformer.h.8.attn.c_proj.weight\", \"decoder.transformer.h.8.attn.c_proj.bias\", \"decoder.transformer.h.8.ln_2.weight\", \"decoder.transformer.h.8.ln_2.bias\", \"decoder.transformer.h.8.mlp.c_fc.weight\", \"decoder.transformer.h.8.mlp.c_fc.bias\", \"decoder.transformer.h.8.mlp.c_proj.weight\", \"decoder.transformer.h.8.mlp.c_proj.bias\", \"decoder.transformer.h.9.ln_1.weight\", \"decoder.transformer.h.9.ln_1.bias\", \"decoder.transformer.h.9.attn.c_attn.weight\", \"decoder.transformer.h.9.attn.c_attn.bias\", \"decoder.transformer.h.9.attn.c_proj.weight\", \"decoder.transformer.h.9.attn.c_proj.bias\", \"decoder.transformer.h.9.ln_2.weight\", \"decoder.transformer.h.9.ln_2.bias\", \"decoder.transformer.h.9.mlp.c_fc.weight\", \"decoder.transformer.h.9.mlp.c_fc.bias\", \"decoder.transformer.h.9.mlp.c_proj.weight\", \"decoder.transformer.h.9.mlp.c_proj.bias\", \"decoder.transformer.h.10.ln_1.weight\", \"decoder.transformer.h.10.ln_1.bias\", \"decoder.transformer.h.10.attn.c_attn.weight\", \"decoder.transformer.h.10.attn.c_attn.bias\", \"decoder.transformer.h.10.attn.c_proj.weight\", \"decoder.transformer.h.10.attn.c_proj.bias\", \"decoder.transformer.h.10.ln_2.weight\", \"decoder.transformer.h.10.ln_2.bias\", \"decoder.transformer.h.10.mlp.c_fc.weight\", \"decoder.transformer.h.10.mlp.c_fc.bias\", \"decoder.transformer.h.10.mlp.c_proj.weight\", \"decoder.transformer.h.10.mlp.c_proj.bias\", \"decoder.transformer.h.11.ln_1.weight\", \"decoder.transformer.h.11.ln_1.bias\", \"decoder.transformer.h.11.attn.c_attn.weight\", \"decoder.transformer.h.11.attn.c_attn.bias\", \"decoder.transformer.h.11.attn.c_proj.weight\", \"decoder.transformer.h.11.attn.c_proj.bias\", \"decoder.transformer.h.11.ln_2.weight\", \"decoder.transformer.h.11.ln_2.bias\", \"decoder.transformer.h.11.mlp.c_fc.weight\", \"decoder.transformer.h.11.mlp.c_fc.bias\", \"decoder.transformer.h.11.mlp.c_proj.weight\", \"decoder.transformer.h.11.mlp.c_proj.bias\", \"decoder.transformer.ln_f.weight\", \"decoder.transformer.ln_f.bias\", \"decoder.lm_head.weight\", \"encoder_to_decoder.weight\", \"encoder_to_decoder.bias\". \n\tUnexpected key(s) in state_dict: \"gpt2.transformer.wte.weight\", \"gpt2.transformer.wpe.weight\", \"gpt2.transformer.h.0.ln_1.weight\", \"gpt2.transformer.h.0.ln_1.bias\", \"gpt2.transformer.h.0.attn.c_attn.weight\", \"gpt2.transformer.h.0.attn.c_attn.bias\", \"gpt2.transformer.h.0.attn.c_proj.weight\", \"gpt2.transformer.h.0.attn.c_proj.bias\", \"gpt2.transformer.h.0.ln_2.weight\", \"gpt2.transformer.h.0.ln_2.bias\", \"gpt2.transformer.h.0.mlp.c_fc.weight\", \"gpt2.transformer.h.0.mlp.c_fc.bias\", \"gpt2.transformer.h.0.mlp.c_proj.weight\", \"gpt2.transformer.h.0.mlp.c_proj.bias\", \"gpt2.transformer.h.1.ln_1.weight\", \"gpt2.transformer.h.1.ln_1.bias\", \"gpt2.transformer.h.1.attn.c_attn.weight\", \"gpt2.transformer.h.1.attn.c_attn.bias\", \"gpt2.transformer.h.1.attn.c_proj.weight\", \"gpt2.transformer.h.1.attn.c_proj.bias\", \"gpt2.transformer.h.1.ln_2.weight\", \"gpt2.transformer.h.1.ln_2.bias\", \"gpt2.transformer.h.1.mlp.c_fc.weight\", \"gpt2.transformer.h.1.mlp.c_fc.bias\", \"gpt2.transformer.h.1.mlp.c_proj.weight\", \"gpt2.transformer.h.1.mlp.c_proj.bias\", \"gpt2.transformer.h.2.ln_1.weight\", \"gpt2.transformer.h.2.ln_1.bias\", \"gpt2.transformer.h.2.attn.c_attn.weight\", \"gpt2.transformer.h.2.attn.c_attn.bias\", \"gpt2.transformer.h.2.attn.c_proj.weight\", \"gpt2.transformer.h.2.attn.c_proj.bias\", \"gpt2.transformer.h.2.ln_2.weight\", \"gpt2.transformer.h.2.ln_2.bias\", \"gpt2.transformer.h.2.mlp.c_fc.weight\", \"gpt2.transformer.h.2.mlp.c_fc.bias\", \"gpt2.transformer.h.2.mlp.c_proj.weight\", \"gpt2.transformer.h.2.mlp.c_proj.bias\", \"gpt2.transformer.h.3.ln_1.weight\", \"gpt2.transformer.h.3.ln_1.bias\", \"gpt2.transformer.h.3.attn.c_attn.weight\", \"gpt2.transformer.h.3.attn.c_attn.bias\", \"gpt2.transformer.h.3.attn.c_proj.weight\", \"gpt2.transformer.h.3.attn.c_proj.bias\", \"gpt2.transformer.h.3.ln_2.weight\", \"gpt2.transformer.h.3.ln_2.bias\", \"gpt2.transformer.h.3.mlp.c_fc.weight\", \"gpt2.transformer.h.3.mlp.c_fc.bias\", \"gpt2.transformer.h.3.mlp.c_proj.weight\", \"gpt2.transformer.h.3.mlp.c_proj.bias\", \"gpt2.transformer.h.4.ln_1.weight\", \"gpt2.transformer.h.4.ln_1.bias\", \"gpt2.transformer.h.4.attn.c_attn.weight\", \"gpt2.transformer.h.4.attn.c_attn.bias\", \"gpt2.transformer.h.4.attn.c_proj.weight\", \"gpt2.transformer.h.4.attn.c_proj.bias\", \"gpt2.transformer.h.4.ln_2.weight\", \"gpt2.transformer.h.4.ln_2.bias\", \"gpt2.transformer.h.4.mlp.c_fc.weight\", \"gpt2.transformer.h.4.mlp.c_fc.bias\", \"gpt2.transformer.h.4.mlp.c_proj.weight\", \"gpt2.transformer.h.4.mlp.c_proj.bias\", \"gpt2.transformer.h.5.ln_1.weight\", \"gpt2.transformer.h.5.ln_1.bias\", \"gpt2.transformer.h.5.attn.c_attn.weight\", \"gpt2.transformer.h.5.attn.c_attn.bias\", \"gpt2.transformer.h.5.attn.c_proj.weight\", \"gpt2.transformer.h.5.attn.c_proj.bias\", \"gpt2.transformer.h.5.ln_2.weight\", \"gpt2.transformer.h.5.ln_2.bias\", \"gpt2.transformer.h.5.mlp.c_fc.weight\", \"gpt2.transformer.h.5.mlp.c_fc.bias\", \"gpt2.transformer.h.5.mlp.c_proj.weight\", \"gpt2.transformer.h.5.mlp.c_proj.bias\", \"gpt2.transformer.h.6.ln_1.weight\", \"gpt2.transformer.h.6.ln_1.bias\", \"gpt2.transformer.h.6.attn.c_attn.weight\", \"gpt2.transformer.h.6.attn.c_attn.bias\", \"gpt2.transformer.h.6.attn.c_proj.weight\", \"gpt2.transformer.h.6.attn.c_proj.bias\", \"gpt2.transformer.h.6.ln_2.weight\", \"gpt2.transformer.h.6.ln_2.bias\", \"gpt2.transformer.h.6.mlp.c_fc.weight\", \"gpt2.transformer.h.6.mlp.c_fc.bias\", \"gpt2.transformer.h.6.mlp.c_proj.weight\", \"gpt2.transformer.h.6.mlp.c_proj.bias\", \"gpt2.transformer.h.7.ln_1.weight\", \"gpt2.transformer.h.7.ln_1.bias\", \"gpt2.transformer.h.7.attn.c_attn.weight\", \"gpt2.transformer.h.7.attn.c_attn.bias\", \"gpt2.transformer.h.7.attn.c_proj.weight\", \"gpt2.transformer.h.7.attn.c_proj.bias\", \"gpt2.transformer.h.7.ln_2.weight\", \"gpt2.transformer.h.7.ln_2.bias\", \"gpt2.transformer.h.7.mlp.c_fc.weight\", \"gpt2.transformer.h.7.mlp.c_fc.bias\", \"gpt2.transformer.h.7.mlp.c_proj.weight\", \"gpt2.transformer.h.7.mlp.c_proj.bias\", \"gpt2.transformer.h.8.ln_1.weight\", \"gpt2.transformer.h.8.ln_1.bias\", \"gpt2.transformer.h.8.attn.c_attn.weight\", \"gpt2.transformer.h.8.attn.c_attn.bias\", \"gpt2.transformer.h.8.attn.c_proj.weight\", \"gpt2.transformer.h.8.attn.c_proj.bias\", \"gpt2.transformer.h.8.ln_2.weight\", \"gpt2.transformer.h.8.ln_2.bias\", \"gpt2.transformer.h.8.mlp.c_fc.weight\", \"gpt2.transformer.h.8.mlp.c_fc.bias\", \"gpt2.transformer.h.8.mlp.c_proj.weight\", \"gpt2.transformer.h.8.mlp.c_proj.bias\", \"gpt2.transformer.h.9.ln_1.weight\", \"gpt2.transformer.h.9.ln_1.bias\", \"gpt2.transformer.h.9.attn.c_attn.weight\", \"gpt2.transformer.h.9.attn.c_attn.bias\", \"gpt2.transformer.h.9.attn.c_proj.weight\", \"gpt2.transformer.h.9.attn.c_proj.bias\", \"gpt2.transformer.h.9.ln_2.weight\", \"gpt2.transformer.h.9.ln_2.bias\", \"gpt2.transformer.h.9.mlp.c_fc.weight\", \"gpt2.transformer.h.9.mlp.c_fc.bias\", \"gpt2.transformer.h.9.mlp.c_proj.weight\", \"gpt2.transformer.h.9.mlp.c_proj.bias\", \"gpt2.transformer.h.10.ln_1.weight\", \"gpt2.transformer.h.10.ln_1.bias\", \"gpt2.transformer.h.10.attn.c_attn.weight\", \"gpt2.transformer.h.10.attn.c_attn.bias\", \"gpt2.transformer.h.10.attn.c_proj.weight\", \"gpt2.transformer.h.10.attn.c_proj.bias\", \"gpt2.transformer.h.10.ln_2.weight\", \"gpt2.transformer.h.10.ln_2.bias\", \"gpt2.transformer.h.10.mlp.c_fc.weight\", \"gpt2.transformer.h.10.mlp.c_fc.bias\", \"gpt2.transformer.h.10.mlp.c_proj.weight\", \"gpt2.transformer.h.10.mlp.c_proj.bias\", \"gpt2.transformer.h.11.ln_1.weight\", \"gpt2.transformer.h.11.ln_1.bias\", \"gpt2.transformer.h.11.attn.c_attn.weight\", \"gpt2.transformer.h.11.attn.c_attn.bias\", \"gpt2.transformer.h.11.attn.c_proj.weight\", \"gpt2.transformer.h.11.attn.c_proj.bias\", \"gpt2.transformer.h.11.ln_2.weight\", \"gpt2.transformer.h.11.ln_2.bias\", \"gpt2.transformer.h.11.mlp.c_fc.weight\", \"gpt2.transformer.h.11.mlp.c_fc.bias\", \"gpt2.transformer.h.11.mlp.c_proj.weight\", \"gpt2.transformer.h.11.mlp.c_proj.bias\", \"gpt2.transformer.ln_f.weight\", \"gpt2.transformer.ln_f.bias\", \"gpt2.lm_head.weight\", \"img_proj.weight\", \"img_proj.bias\". \n       Custom model evaluation might fail or use only pre-trained weights.\n\nInference helper functions defined.\nSmolVLM model ready: Yes\nCustom model ready: Yes\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def evaluate_on_occluded_images(model,\n                                dataloader,\n                                device,\n                                occlusion_levels,\n                                is_smolvlm=True,          # Flag to determine model type\n                                smol_processor=None,      # Required if is_smolvlm=True\n                                custom_tokenizer=None,    # Required if is_smolvlm=False\n                                custom_img_transform=None # Required if is_smolvlm=False\n                               ):\n    \"\"\"\n    Evaluates a model's captioning performance on images with varying occlusion levels.\n\n    Args:\n        model (torch.nn.Module): The image captioning model (SmolVLM or Custom).\n        dataloader (DataLoader): DataLoader providing batches of\n                                 (pil_images, captions, transformed_tensors, filenames).\n        device (str): Device ('cuda' or 'cpu').\n        occlusion_levels (list): List of percentages for occlusion (e.g., [10, 50, 80]).\n        is_smolvlm (bool): True if evaluating SmolVLM, False for the custom model.\n        smol_processor: The processor for SmolVLM (needed if is_smolvlm=True).\n        custom_tokenizer: The tokenizer for the custom model (needed if is_smolvlm=False).\n        custom_img_transform: The image transformation function for the custom model\n                             (needed if is_smolvlm=False to process occluded images).\n\n    Returns:\n        tuple: (\n            dict: Dictionary mapping occlusion levels (including 0) to average scores.\n                  Example: {0: {'BLEU': 0.X, ...}, 10: {'BLEU': 0.Y, ...}, ...}\n            list: List of dictionaries containing data for Part C analysis.\n                  Each dict: {'original_caption': str, 'generated_caption': str,\n                              'perturbation_percentage': int, 'filename': str,\n                              'model_type': str} # model_type added outside this function\n        )\n    \"\"\"\n    if is_smolvlm and smol_processor is None:\n        raise ValueError(\"SmolVLM processor is required when is_smolvlm is True.\")\n    if not is_smolvlm and (custom_tokenizer is None or custom_img_transform is None):\n        raise ValueError(\"Custom tokenizer and image transform are required when is_smolvlm is False.\")\n\n    model.eval() # Ensure model is in evaluation mode\n\n    # Store results per level {level: {metric: [list_of_scores]}}\n    results_list = {level: {'BLEU': [], 'METEOR': [], 'ROUGE-L': []} for level in [0] + occlusion_levels}\n    part_c_data_list = [] # Store data for the classifier task\n\n    # --- Iterate through each required occlusion level (plus 0% baseline) ---\n    for level in [0] + occlusion_levels:\n        print(f\"\\n--- Evaluating Occlusion Level: {level}% ---\")\n\n        # --- Iterate through batches from the dataloader ---\n        for batch_data in tqdm(dataloader, desc=f\"Level {level}% Batches\"):\n            if batch_data is None or batch_data[0] is None: # Check if batch is valid (collate_fn returns None on empty)\n                # print(\"Skipping empty or invalid batch.\")\n                continue\n\n            pil_images, gt_captions, transformed_batch, filenames = batch_data\n\n            # --- Process each item within the batch ---\n            for i in range(len(pil_images)):\n                original_pil = pil_images[i]\n                gt_caption = gt_captions[i]\n                filename = filenames[i]\n\n                # --- Prepare Image Input based on Occlusion Level and Model Type ---\n                try:\n                    if level == 0:\n                        # No occlusion: Use original PIL for SmolVLM, pre-transformed tensor for Custom\n                        if is_smolvlm:\n                            input_image_for_model = original_pil\n                        else:\n                            if transformed_batch is None or i >= len(transformed_batch):\n                                 # print(f\"Warning: Missing transformed tensor for {filename} at level 0. Skipping.\")\n                                 continue # Skip if transform failed earlier for this item\n                            input_image_for_model = transformed_batch[i] # Already a tensor\n                    else:\n                        # Apply occlusion\n                        np_image = np.array(original_pil)\n                        occluded_np_image = occlude_image(np_image, level)\n                        occluded_pil_image = Image.fromarray(occluded_np_image) # Convert back to PIL\n\n                        if is_smolvlm:\n                            input_image_for_model = occluded_pil_image # SmolVLM takes PIL\n                        else:\n                            # Custom model needs the occluded PIL image transformed\n                            input_image_for_model = custom_img_transform(occluded_pil_image) # Apply transform\n\n                except Exception as e:\n                    print(f\"Error processing/occluding image {filename} at level {level}%: {e}. Skipping sample.\")\n                    continue # Skip this sample if occlusion/processing fails\n\n                # --- Generate Caption ---\n                pred_caption = \"\" # Initialize in case generation fails\n                try:\n                    if is_smolvlm:\n                        if smol_model: # Check if model loaded successfully\n                             pred_caption = generate_caption_smolvlm(input_image_for_model, smol_model, smol_processor, device)\n                    else:\n                        if custom_model: # Check if model loaded successfully\n                             # Pass the transformed tensor (either original or occluded+transformed)\n                             pred_caption = generate_caption_custom(input_image_for_model, custom_model, custom_tokenizer, device)\n                except Exception as e:\n                     print(f\"Error during caption generation for {filename} at level {level}%: {e}\")\n                     # pred_caption remains \"\"\n\n\n                # --- Calculate Metrics ---\n                bleu_score, meteor_score_val, rouge_l_score = 0.0, 0.0, 0.0 # Default scores\n                if gt_caption and pred_caption: # Only calculate if both are non-empty\n                    try:\n                        # Tokenize for BLEU and METEOR\n                        ref_tokens = word_tokenize(gt_caption.lower())\n                        pred_tokens = word_tokenize(pred_caption.lower())\n\n                        # Avoid errors with empty lists after tokenization\n                        if ref_tokens and pred_tokens:\n                            # BLEU Score\n                            bleu_score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SMOOTHIE)\n\n                            # METEOR Score (requires string inputs)\n                            try:\n                                meteor_score_val = meteor_score([' '.join(ref_tokens)], ' '.join(pred_tokens))\n                            except Exception as meteor_err:\n                                # print(f\"Meteor calc error for {filename}: {meteor_err}\")\n                                meteor_score_val = 0.0 # Assign 0 if calculation fails\n\n                            # ROUGE-L Score (requires string inputs)\n                            try:\n                                rouge_scores = ROUGE.score(gt_caption, pred_caption)\n                                rouge_l_score = rouge_scores['rougeL'].fmeasure\n                            except Exception as rouge_err:\n                                # print(f\"Rouge calc error for {filename}: {rouge_err}\")\n                                rouge_l_score = 0.0 # Assign 0 if calculation fails\n                        # else: print(f\"Warning: Empty tokens for {filename}\") # Optional debug\n\n                    except Exception as metric_err:\n                        # Catch any other unexpected metric errors\n                        # print(f\"General metric calculation error for {filename}: {metric_err}\")\n                        bleu_score, meteor_score_val, rouge_l_score = 0.0, 0.0, 0.0\n\n                # Append scores for the current level\n                results_list[level]['BLEU'].append(bleu_score)\n                results_list[level]['METEOR'].append(meteor_score_val)\n                results_list[level]['ROUGE-L'].append(rouge_l_score)\n\n                # --- Store Data for Part C (only for specified occlusion levels > 0) ---\n                if level in occlusion_levels: # Store only for 10, 50, 80% etc.\n                    part_c_data_list.append({\n                        'original_caption': gt_caption,\n                        'generated_caption': pred_caption if pred_caption else \"\", # Ensure empty string if generation failed\n                        'perturbation_percentage': level,\n                        'filename': filename\n                        # 'model_type' will be added after calling this function\n                    })\n        # --- End of Batch Loop ---\n    # --- End of Occlusion Level Loop ---\n\n    # --- Calculate Average Scores ---\n    avg_results = {level: {} for level in [0] + occlusion_levels}\n    for level in results_list:\n        for metric in results_list[level]:\n            scores = results_list[level][metric]\n            avg_results[level][metric] = np.mean(scores) if scores else 0.0\n        print(f\"Level {level}% Avg Scores: \"\n              f\"BLEU={avg_results[level]['BLEU']:.4f}, \"\n              f\"METEOR={avg_results[level]['METEOR']:.4f}, \"\n              f\"ROUGE-L={avg_results[level]['ROUGE-L']:.4f}\")\n\n\n    return avg_results, part_c_data_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:28:07.864282Z","iopub.execute_input":"2025-04-14T13:28:07.864892Z","iopub.status.idle":"2025-04-14T13:28:07.879670Z","shell.execute_reply.started":"2025-04-14T13:28:07.864869Z","shell.execute_reply":"2025-04-14T13:28:07.878852Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# PART 1: SmolVLM Evaluation Execution\n\nprint(\"\\nExecuting Part B - Step 1: SmolVLM Evaluation\")\n\n# --- Helper Function to Calculate Performance Changes ---\n# (Include this helper in both parts or define it globally earlier)\ndef calculate_changes(results_raw):\n    changes = {level: {} for level in OCCLUSION_LEVELS}\n    baseline = results_raw.get(0, {'BLEU': 0.0, 'METEOR': 0.0, 'ROUGE-L': 0.0})\n    for level in OCCLUSION_LEVELS:\n        current = results_raw.get(level, {'BLEU': 0.0, 'METEOR': 0.0, 'ROUGE-L': 0.0})\n        changes[level]['BLEU_change'] = current.get('BLEU', 0.0) - baseline.get('BLEU', 0.0)\n        changes[level]['METEOR_change'] = current.get('METEOR', 0.0) - baseline.get('METEOR', 0.0)\n        changes[level]['ROUGE-L_change'] = current.get('ROUGE-L', 0.0) - baseline.get('ROUGE-L', 0.0)\n    return changes, baseline\n\n# --- Run Evaluation for SmolVLM ---\nprint(\"\\n===== Evaluating SmolVLM Robustness =====\")\nsmolvlm_results_raw, smolvlm_part_c_data = ({}, []) # Initialize defaults\nif 'smol_model' in globals() and smol_model and 'smol_processor' in globals() and smol_processor:\n    smolvlm_results_raw, smolvlm_part_c_data = evaluate_on_occluded_images(\n        model=smol_model,\n        dataloader=test_dataloader, # Assumes test_dataloader is defined\n        device=DEVICE,             # Assumes DEVICE is defined\n        occlusion_levels=OCCLUSION_LEVELS, # Assumes OCCLUSION_LEVELS is defined\n        is_smolvlm=True,\n        smol_processor=smol_processor,\n        custom_tokenizer=None,      # Not needed\n        custom_img_transform=None   # Not needed\n    )\n    # Add model type label for Part C\n    for item in smolvlm_part_c_data:\n        item['model_type'] = 'SmolVLM'\n    print(\"SmolVLM evaluation complete.\")\nelse:\n    print(\"Skipping SmolVLM evaluation: Model or Processor not loaded.\")\n\n# --- Calculate and Report SmolVLM Changes ---\nprint(\"\\n--- Calculating SmolVLM Performance Changes ---\")\nsmolvlm_changes, smolvlm_baseline = calculate_changes(smolvlm_results_raw)\n\nprint(\"\\n--- SmolVLM Baseline Performance (0% Occlusion) ---\")\nprint(f\"BLEU={smolvlm_baseline.get('BLEU', 0.0):.4f}, METEOR={smolvlm_baseline.get('METEOR', 0.0):.4f}, ROUGE-L={smolvlm_baseline.get('ROUGE-L', 0.0):.4f}\")\n\nprint(\"\\n--- SmolVLM Performance Change (Score_at_Occlusion - Score_Baseline) ---\")\nprint(\"-\" * 68)\nprint(f\"{'Model':<11} | {'Occlusion':^9} | {'BLEU Change':^11} | {'METEOR Change':^13} | {'ROUGE-L Change':^14}\")\nprint(\"-\" * 68)\nfor level in OCCLUSION_LEVELS:\n    s_chg = smolvlm_changes.get(level, {})\n    print(f\"{'SmolVLM':<11} | {level:^9}% | {s_chg.get('BLEU_change', 0.0):^11.4f} | {s_chg.get('METEOR_change', 0.0):^13.4f} | {s_chg.get('ROUGE-L_change', 0.0):^14.4f}\")\nprint(\"-\" * 68)\n\n# --- Save SmolVLM Data for Part C ---\nif smolvlm_part_c_data:\n    smolvlm_part_c_df = pd.DataFrame(smolvlm_part_c_data)\n    part_c_output_dir = \"/content/drive/MyDrive/assignment 2 deep learning/results\" # Define output directory\n    os.makedirs(part_c_output_dir, exist_ok=True)\n    smolvlm_output_path = os.path.join(part_c_output_dir, \"part_b_smolvlm_results_for_part_c.csv\")\n    try:\n        smolvlm_part_c_df.to_csv(smolvlm_output_path, index=False)\n        print(f\"\\nSuccessfully saved {len(smolvlm_part_c_df)} SmolVLM results for Part C to:\\n{smolvlm_output_path}\")\n    except Exception as e:\n        print(f\"\\nERROR: Failed to save SmolVLM Part C data to {smolvlm_output_path}: {e}\")\nelse:\n    print(\"\\nWarning: No SmolVLM data collected for Part C.\")\n\nprint(\"\\nPart B - Step 1 (SmolVLM Evaluation) Finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:28:36.018411Z","iopub.execute_input":"2025-04-14T11:28:36.018762Z","iopub.status.idle":"2025-04-14T12:38:10.238842Z","shell.execute_reply.started":"2025-04-14T11:28:36.018734Z","shell.execute_reply":"2025-04-14T12:38:10.237977Z"}},"outputs":[{"name":"stdout","text":"\nExecuting Part B - Step 1: SmolVLM Evaluation\n\n===== Evaluating SmolVLM Robustness =====\n\n--- Evaluating Occlusion Level: 0% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 0% Batches: 100%|██████████| 25/25 [17:39<00:00, 42.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 10% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 10% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 10% Batches: 100%|██████████| 25/25 [18:02<00:00, 43.31s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 50% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 50% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 50% Batches: 100%|██████████| 25/25 [17:28<00:00, 41.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 80% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 80% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 80% Batches: 100%|██████████| 25/25 [16:22<00:00, 39.31s/it]","output_type":"stream"},{"name":"stdout","text":"Level 0% Avg Scores: BLEU=0.0576, METEOR=0.0000, ROUGE-L=0.2545\nLevel 10% Avg Scores: BLEU=0.0471, METEOR=0.0000, ROUGE-L=0.2396\nLevel 50% Avg Scores: BLEU=0.0369, METEOR=0.0000, ROUGE-L=0.2228\nLevel 80% Avg Scores: BLEU=0.0204, METEOR=0.0000, ROUGE-L=0.1875\nSmolVLM evaluation complete.\n\n--- Calculating SmolVLM Performance Changes ---\n\n--- SmolVLM Baseline Performance (0% Occlusion) ---\nBLEU=0.0576, METEOR=0.0000, ROUGE-L=0.2545\n\n--- SmolVLM Performance Change (Score_at_Occlusion - Score_Baseline) ---\n--------------------------------------------------------------------\nModel       | Occlusion | BLEU Change | METEOR Change | ROUGE-L Change\n--------------------------------------------------------------------\nSmolVLM     |    10    % |   -0.0105   |    0.0000     |    -0.0148    \nSmolVLM     |    50    % |   -0.0206   |    0.0000     |    -0.0317    \nSmolVLM     |    80    % |   -0.0372   |    0.0000     |    -0.0669    \n--------------------------------------------------------------------\n\nSuccessfully saved 600 SmolVLM results for Part C to:\n/content/drive/MyDrive/assignment 2 deep learning/results/part_b_smolvlm_results_for_part_c.csv\n\nPart B - Step 1 (SmolVLM Evaluation) Finished.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# PART 2: Custom Model Evaluation Execution\n\nprint(\"\\nExecuting Part B - Step 2: Custom Model Evaluation\")\n\n# --- Helper Function to Calculate Performance Changes ---\n# (Ensure this function is defined in your notebook, either here or globally before)\ndef calculate_changes(results_raw):\n    \"\"\"Calculates the difference between baseline (0%) and other occlusion levels.\"\"\"\n    # Default baseline if level 0 is missing for some reason\n    baseline = results_raw.get(0, {'BLEU': 0.0, 'METEOR': 0.0, 'ROUGE-L': 0.0})\n    # Ensure OCCLUSION_LEVELS is defined (e.g., [10, 50, 80])\n    if 'OCCLUSION_LEVELS' not in globals(): raise NameError(\"OCCLUSION_LEVELS not defined.\")\n    changes = {level: {} for level in OCCLUSION_LEVELS}\n\n    for level in OCCLUSION_LEVELS:\n        # Default scores for the level if it's missing\n        current = results_raw.get(level, {'BLEU': 0.0, 'METEOR': 0.0, 'ROUGE-L': 0.0})\n        changes[level]['BLEU_change'] = current.get('BLEU', 0.0) - baseline.get('BLEU', 0.0)\n        changes[level]['METEOR_change'] = current.get('METEOR', 0.0) - baseline.get('METEOR', 0.0)\n        changes[level]['ROUGE-L_change'] = current.get('ROUGE-L', 0.0) - baseline.get('ROUGE-L', 0.0)\n    return changes, baseline\n\n# --- Check Prerequisites ---\n# Verify that the necessary components from previous steps exist\n# These checks run *before* attempting the potentially long evaluation loop\nmodel_ready = 'custom_model' in globals() and custom_model is not None\ntokenizer_ready = 'custom_tokenizer' in globals() and custom_tokenizer is not None\ntransform_ready = 'custom_transform' in globals() and custom_transform is not None\ndataloader_ready = 'test_dataloader' in globals() and test_dataloader is not None\nconstants_ready = ('DEVICE' in globals() and 'OCCLUSION_LEVELS' in globals())\n\nprerequisites_met = model_ready and tokenizer_ready and transform_ready and dataloader_ready and constants_ready\n\nif not prerequisites_met:\n    print(\"\\nERROR: Prerequisites for Custom Model evaluation not met.\")\n    print(f\"  - Custom Model Loaded: {model_ready}\")\n    print(f\"  - Custom Tokenizer Loaded: {tokenizer_ready}\")\n    print(f\"  - Custom Transform Defined: {transform_ready}\")\n    print(f\"  - Test DataLoader Ready: {dataloader_ready}\")\n    print(f\"  - DEVICE/OCCLUSION_LEVELS Defined: {constants_ready}\")\n    print(\"Skipping Custom Model evaluation.\")\nelse:\n    print(\"\\nPrerequisites met. Starting Custom Model evaluation...\")\n    # --- Run Evaluation for Custom Model ---\n    print(\"===== Evaluating Custom Model Robustness =====\")\n    custom_model_results_raw, custom_model_part_c_data = ({}, []) # Initialize defaults\n\n    try:\n        custom_model_results_raw, custom_model_part_c_data = evaluate_on_occluded_images(\n            model=custom_model,\n            dataloader=test_dataloader,\n            device=DEVICE,\n            occlusion_levels=OCCLUSION_LEVELS,\n            is_smolvlm=False,                    # Flag for custom model logic\n            custom_tokenizer=custom_tokenizer,\n            custom_img_transform=custom_transform,\n            smol_processor=None                  # Not needed for custom model\n        )\n        # Add model type label for Part C data\n        for item in custom_model_part_c_data:\n            item['model_type'] = 'Custom'\n        print(\"Custom model evaluation run finished.\")\n\n    except Exception as eval_error:\n        print(f\"\\nERROR during custom model evaluation run: {eval_error}\")\n        print(\"Evaluation may be incomplete.\")\n        # Results might be partially filled or empty\n\n\n    # --- Calculate and Report Custom Model Changes ---\n    custom_model_changes, custom_model_baseline = (None, None) # Initialize\n    if custom_model_results_raw: # Check if the results dict has data\n        print(\"\\n--- Calculating Custom Model Performance Changes ---\")\n        try:\n             custom_model_changes, custom_model_baseline = calculate_changes(custom_model_results_raw)\n\n             print(\"\\n--- Custom Model Baseline Performance (0% Occlusion) ---\")\n             print(f\"BLEU={custom_model_baseline.get('BLEU', 0.0):.4f}, METEOR={custom_model_baseline.get('METEOR', 0.0):.4f}, ROUGE-L={custom_model_baseline.get('ROUGE-L', 0.0):.4f}\")\n\n             print(\"\\n--- Custom Model Performance Change (Score_at_Occlusion - Score_Baseline) ---\")\n             print(\"-\" * 68)\n             print(f\"{'Model':<11} | {'Occlusion':^9} | {'BLEU Change':^11} | {'METEOR Change':^13} | {'ROUGE-L Change':^14}\")\n             print(\"-\" * 68)\n             if custom_model_changes:\n                 for level in OCCLUSION_LEVELS:\n                     c_chg = custom_model_changes.get(level, {})\n                     print(f\"{'Custom':<11} | {level:^9}% | {c_chg.get('BLEU_change', 0.0):^11.4f} | {c_chg.get('METEOR_change', 0.0):^13.4f} | {c_chg.get('ROUGE-L_change', 0.0):^14.4f}\")\n             else:\n                 # Should not happen if results_raw is not empty, but included for safety\n                 print(f\"{'Custom':<11} | {'---':^9} | {'---':^11} | {'---':^13} | {'---':^14} (Calculation Failed)\")\n             print(\"-\" * 68)\n        except Exception as report_err:\n             print(f\"\\nERROR during results calculation/reporting: {report_err}\")\n\n    else:\n        print(\"\\nSkipping Custom Model results reporting as evaluation results are empty or evaluation failed.\")\n\n\n    # --- Save Custom Model Data for Part C ---\n    if custom_model_part_c_data: # Check if any data was collected\n        custom_part_c_df = pd.DataFrame(custom_model_part_c_data)\n        # Ensure output directory exists\n        part_c_output_dir = \"/kaggle/working/results\" # Saving to working directory is often easier in Kaggle\n        # Or use: part_c_output_dir = \"/content/drive/MyDrive/assignment 2 deep learning/results\" # If using Drive\n        try:\n            os.makedirs(part_c_output_dir, exist_ok=True)\n            custom_output_path = os.path.join(part_c_output_dir, \"part_b_custom_results_for_part_c.csv\")\n            custom_part_c_df.to_csv(custom_output_path, index=False)\n            print(f\"\\nSuccessfully saved {len(custom_part_c_df)} Custom Model results for Part C to:\\n{custom_output_path}\")\n        except Exception as e:\n            print(f\"\\nERROR: Failed to save Custom Model Part C data to {custom_output_path}: {e}\")\n    else:\n        print(\"\\nWarning: No Custom Model data collected/saved for Part C (check evaluation logs for errors).\")\n\nprint(\"\\nPart B - Step 2 (Custom Model Evaluation Script) Finished.\")\n\n# Reminder about combining CSVs if needed later\n# print(\"\\nNote: If SmolVLM evaluation also ran successfully, remember to combine \")\n# print(\" 'part_b_smolvlm_results_for_part_c.csv' and 'part_b_custom_results_for_part_c.csv' for Part C input.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T13:28:25.554805Z","iopub.execute_input":"2025-04-14T13:28:25.555555Z","iopub.status.idle":"2025-04-14T13:32:10.618479Z","shell.execute_reply.started":"2025-04-14T13:28:25.555524Z","shell.execute_reply":"2025-04-14T13:32:10.617755Z"}},"outputs":[{"name":"stdout","text":"\nExecuting Part B - Step 2: Custom Model Evaluation\n\nPrerequisites met. Starting Custom Model evaluation...\n===== Evaluating Custom Model Robustness =====\n\n--- Evaluating Occlusion Level: 0% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 0% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 0% Batches: 100%|██████████| 25/25 [00:55<00:00,  2.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 10% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 10% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 10% Batches: 100%|██████████| 25/25 [00:56<00:00,  2.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 50% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 50% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 50% Batches: 100%|██████████| 25/25 [00:56<00:00,  2.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Occlusion Level: 80% ---\n","output_type":"stream"},{"name":"stderr","text":"Level 80% Batches:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nLevel 80% Batches: 100%|██████████| 25/25 [00:56<00:00,  2.27s/it]","output_type":"stream"},{"name":"stdout","text":"Level 0% Avg Scores: BLEU=0.0081, METEOR=0.0000, ROUGE-L=0.1343\nLevel 10% Avg Scores: BLEU=0.0083, METEOR=0.0000, ROUGE-L=0.1317\nLevel 50% Avg Scores: BLEU=0.0127, METEOR=0.0000, ROUGE-L=0.1525\nLevel 80% Avg Scores: BLEU=0.0128, METEOR=0.0000, ROUGE-L=0.1482\nCustom model evaluation run finished.\n\n--- Calculating Custom Model Performance Changes ---\n\n--- Custom Model Baseline Performance (0% Occlusion) ---\nBLEU=0.0081, METEOR=0.0000, ROUGE-L=0.1343\n\n--- Custom Model Performance Change (Score_at_Occlusion - Score_Baseline) ---\n--------------------------------------------------------------------\nModel       | Occlusion | BLEU Change | METEOR Change | ROUGE-L Change\n--------------------------------------------------------------------\nCustom      |    10    % |   0.0002    |    0.0000     |    -0.0026    \nCustom      |    50    % |   0.0045    |    0.0000     |     0.0182    \nCustom      |    80    % |   0.0047    |    0.0000     |     0.0139    \n--------------------------------------------------------------------\n\nSuccessfully saved 600 Custom Model results for Part C to:\n/kaggle/working/results/part_b_custom_results_for_part_c.csv\n\nPart B - Step 2 (Custom Model Evaluation Script) Finished.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}